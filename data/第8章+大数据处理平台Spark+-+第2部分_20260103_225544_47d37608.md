---
source: /Users/hinrayleung/Documents/BNU/dataAna/PPT/第8章+大数据处理平台Spark+-+第2部分.pdf
type: pdf
model: mistral-ocr-latest
processed: 2026-01-03T22:55:44.141257
---

# Document: 第8章+大数据处理平台Spark+-+第2部分.pdf

## Table of Contents

- [Page 1](#page-1)
- [Page 2](#page-2)
- [Page 3](#page-3)
- [Page 4](#page-4)
- [Page 5](#page-5)
- [Page 6](#page-6)
- [Page 7](#page-7)
- [Page 8](#page-8)
- [Page 9](#page-9)
- [Page 10](#page-10)
- [Page 11](#page-11)
- [Page 12](#page-12)
- [Page 13](#page-13)
- [Page 14](#page-14)
- [Page 15](#page-15)
- [Page 16](#page-16)
- [Page 17](#page-17)
- [Page 18](#page-18)
- [Page 19](#page-19)
- [Page 20](#page-20)
- [Page 21](#page-21)
- [Page 22](#page-22)
- [Page 23](#page-23)
- [Page 24](#page-24)
- [Page 25](#page-25)
- [Page 26](#page-26)
- [Page 27](#page-27)
- [Page 28](#page-28)
- [Page 29](#page-29)
- [Page 30](#page-30)
- [Page 31](#page-31)
- [Page 32](#page-32)
- [Page 33](#page-33)
- [Page 34](#page-34)
- [Page 35](#page-35)
- [Page 36](#page-36)
- [Page 37](#page-37)
- [Page 38](#page-38)
- [Page 39](#page-39)
- [Page 40](#page-40)
- [Page 41](#page-41)
- [Page 42](#page-42)
- [Page 43](#page-43)
- [Page 44](#page-44)
- [Page 45](#page-45)
- [Page 46](#page-46)
- [Page 47](#page-47)
- [Page 48](#page-48)
- [Page 49](#page-49)
- [Page 50](#page-50)
- [Page 51](#page-51)
- [Page 52](#page-52)
- [Page 53](#page-53)
- [Page 54](#page-54)
- [Page 55](#page-55)
- [Page 56](#page-56)
- [Page 57](#page-57)
- [Page 58](#page-58)
- [Page 59](#page-59)
- [Page 60](#page-60)
- [Page 61](#page-61)
- [Page 62](#page-62)
- [Page 63](#page-63)
- [Page 64](#page-64)
- [Page 65](#page-65)
- [Page 66](#page-66)
- [Page 67](#page-67)
- [Page 68](#page-68)
- [Page 69](#page-69)
- [Page 70](#page-70)
- [Page 71](#page-71)
- [Page 72](#page-72)
- [Page 73](#page-73)
- [Page 74](#page-74)
- [Page 75](#page-75)
- [Page 76](#page-76)
- [Page 77](#page-77)

## Page 1

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 第8章 大数据处理平台Spark

## Page 2

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 主要内容

8.1 Spark概述
8.2 Spark基础
8.3 Spark RDD
8.4 Spark SQL
8.5 Spark流计算
8.6 Spark机器学习

2

## Page 3

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.5 Spark流计算

- 8.5.1 Spark Streaming
- 8.5.2 Structured Streaming

3

## Page 4

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.5.1 Spark Streaming

## 设计思想

- 微批处理：Spark Streaming接收实时数据流并分成很小的batch，然后将这些batch交给Spark引擎并以微批量数据的形式进行处理

![img-0.jpeg](img-0.jpeg)

## Page 5

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.5.1 Spark Streaming

## 体系架构

- 驱动器：对SparkContext进行了扩充，构造了StreamingContext对象，包含用于管理流计算的元数据
- 执行器：增加了接收者（Receiver）任务负责从外部数据源持续获取流数据

![img-1.jpeg](img-1.jpeg)

## Page 6

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.5.1 Spark Streaming

## 数据源

- Spark Streaming用DStream来抽象表示连续的数据流
- 在Spark内部，DStream表示为一系列RDD
- Spark Streaming提供了两种数据源
- 基本数据源：StreamingContext API原生支持的数据源
- socketTextStream
- textFileStream
- fileStream
- queueStream
- 高级数据源：例如Kafka、Flume、Kinesis等数据源，可通过扩展实现其他依赖

6

## Page 7

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.5.1 Spark Streaming

## DStream操作

- 针对DStream的任何操作最终都会转换为RDD操作
- DStream操作隐藏了大部分细节，并为开发人员提供了更高级别的API以调用
- DStream既可以从外部数据流接入生成，也可由其他DStream转换而来

7

## Page 8

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

8

# 8.5.1 Spark Streaming

## DStream操作

- DStream操作可以分为
- 普通转换操作
- 窗口转换操作
- 输出操作

## Page 9

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

普通转换操作

## 程序示例

- 每10s统计1次各单词出现的次数

```java
package sparkdemo.streaming;
import org.apache.spark.;
import org.apache.spark.sql.;
import org.apache.spark.streaming.;
```

$\odot$ object Test

```java
def main(args: Array[String]): Unit = {
val conf = new SparkConf().setMaster("local[2]").setAppName("demo");
conf.set("spark.testing.memory", "1073741824");
val ssc = new StreamingContext(conf, Seconds(10));
val lines = ssc.socketTextStream("localhost", 9999);
val words = lines.flatMap(_.split(" "));
val pairs = words.map(x =&gt; (x, 1));
val wordCounts = pairs.reduceByKey(_ + _);
wordCounts.print();
System.out.println("wordcount: " + wordCounts);
ssc.start();
ssc.awaitTermination();
}
}

## Page 10

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

窗口转换操作

## 几个基础概念

- 批处理间隔
- Spark定期将批处理时间间隔内接收到的数据收集起来，组成新的微批数据并提交给Spark引擎处理

- 窗口间隔
- 窗口的持续时间，只有当窗口时间间隔满足条件时才会触发窗口转换操作

- 滑动间隔
- 表示经过多长时间窗口就滑动一次，从而形成新的窗口

- 注意：滑动间隔和窗口间隔的大小需设置为批处理间隔的整数倍

10

## Page 11

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

窗口转换操作

## 窗口示例

- 窗口间隔：3个时间单位
- 滑动间隔：2个时间单位

![img-2.jpeg](img-2.jpeg)

## Page 12

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

窗口转换操作

## 常用的窗口转换操作

- window(windowLength, slideInterval): 基于源DStream产生的窗口化的批数据，计算得到一个新的Dstream
- countByWindow(windowLength, slideInterval): 统计滑动窗口内DStream中元素的数量
- countByValueAndWindow(windowLength, slideInterval, [numTasks]): 当应用到一个(K,V)键值对组成的DStream上，返回一个由(K,V)键值对组成的新的DStream。每个K对应的V是它们在滑动窗口中出现的频率

12

## Page 13

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

窗口转换操作

## 常用的窗口转换操作

- **reduceByKeyAndWindow**(func, windowLength, slideInterval, [numTasks]): 应用到一个(K,V)键值对组成的DStream上时，会返回一个由(K,V)键值对组成的新的DStream。每一个K对应的V均由给定的reduce函数(func函数)进行聚合计算。
- 注意：在默认情况下，这个算子利用了Spark默认的并发任务数去分组。可以通过numTasks参数的设置来指定不同的任务数

- **reduceByKeyAndWindow**(func, invFunc, windowLength, slideInterval, [numTasks]): 更加高效的 reduceByKeyAndWindow，每个窗口的reduce值，是基于先前窗口的reduce值进行增量计算得到的；它会对进入滑动窗口的新数据进行reduce操作，并对离开窗口的老数据进行“逆向reduce”操作。但是，只能用于“可逆reduce函数”，即那些reduce函数都有一个对应的“逆向reduce函数”（以InvFunc参数传入）

13

## Page 14

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

窗口转换操作

## 示例

- 每20s统计一次近30s内各单词出现的次数

```java
package sparkdemo.streaming;
import org.apache.spark.;
import org.apache.spark.sql.;
import org.apache.spark.streaming.;
```

```java
object Test
def main(args: Array[String]): Unit = {
val conf = new SparkConf().setMaster("local[2]").setAppName("demo");
conf.set("spark.testing.memory", "1073741824");
val ssc = new StreamingContext(conf, Seconds(10));
ssc.checkpoint("/checkpoint");
val lines = ssc.socketTextStream("localhost", 9999);
val words = lines.flatMap(_.split(" "));
val pairs = words.map(x =&gt; (x, 1);
// val wordCounts = pairs.reduceByKey(_ + _);
val wordCounts = pairs.reduceByKeyAndWindow(_ + _, _ - _, Seconds(30), Seconds(20));
wordCounts.print();
System.out.println("wordcount:" + wordCounts);
ssc.start();
ssc.awaitTermination();
}
}
```

14

## Page 15

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

窗口转换操作

需要在跨批次之间维护状态时，就必须使用updateStateByKey操作

- 以WordCount为例
- 对于有状态转化操作而言，本批次的词频统计，会在之前批次的词频统计结果的基础上进行不断累加，所以，最终统计得到的词频，是所有批次的单词的总的词频统计结果

15

## Page 16

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

窗口转换操作

## 程序示例

- 统计自程序运行以来各单词出现的次数

```java
package sparkdemo.streaming;
import org.apache.spark.;
import org.apache.spark.sql.;
import org.apache.spark.streaming.;
```

- object Test {
def main(args: Array[String]): Unit = {
val updateFunc = (values: Seq[Int], state: Option[Int]) =&gt; {
val currentCount = values.foldLeft(0)(_ + _);
val previousCount = state.getOrElse(0);
Some(currentCount + previousCount);
};
val conf = new SparkConf().setMaster("local[2]").setAppName("demo");
conf.set("spark.testing.memory", "1073741824");
val ssc = new StreamingContext(conf, Seconds(10));
ssc.checkpoint("/checkpoint");
val lines = ssc.socketTextStream("localhost", 9999);
val words = lines.flatMap(_.split(" "));
val pairs = words.map(x =&gt; (x, 1));
val wordCounts = pairs.updateStateByKey[Int](updateFunc);
wordCounts.print();
System.out.println("wordcount:" + wordCounts);
ssc.start();
ssc.awaitTermination();
}
}

## Page 17

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

输出操作

# 常用的DStream输出方法

- print:在驱动器中输出DStream中数据的前10个元素
- saveAsTextFiles: 将DStream中的数据以文本格式输出
- saveAsObjectFiles: 将DStream中的数据序列化并以序列文件格式保存
- saveAsHadoopFiles: 将DStream中的数据以文本格式保存在Hadoop文件中
- foreachRDD(func): 对DStream中每个RDD的数据调用func()函数

17

## Page 18

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

8.5.1 Spark Streaming

- 容错机制
- 基于RDD Lineage的容错
- 基于日志的容错
- 基于检查点的容错
- 端到端的容错语义

18

## Page 19

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 基于RDD Lineage的容错

如果某个执行器发生故障，并且该执行器不含Receiver任务，则表示只有负责数据处理的任务发生了故障

- 可以利用RDD Lineage机制进行容错

19

## Page 20

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 基于日志的容错

如果含有Receiver的执行器发生故障，则表示接收到的输入数据已丢失

- 对从外部数据源直接获取的数据，Spark Streaming将其备份在两个工作节点中。如果执行器发生故障，则从备份节点重新获取数据
- 对从外部存储系统周期地获取的数据，如果执行器发生故障，则直接从外部存储系统重新获取数据

Spark Streaming的Receiver需使用日志记录已经获取的数据

- 当执行器发生故障重启时，可以根据日志来确定需要重新读取哪些数据

20

## Page 21

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 基于检查点的容错

## Spark Streaming的检查点包括:

- 数据检查点
- 旨在加快执行器发生故障后的恢复过程
- 元数据检查点
- 旨在保证驱动器能够从故障中恢复到正常状态
- 元数据检查点的内容包括:
- 配置信息：创建Spark Streaming应用程序的配置信息
- DStream的操作信息：定义应用程序计算逻辑的DStream操作的信息
- 未处理的批次数据的信息：正在排队尚未处理的批次数据的信息

21

## Page 22

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

端到端的容错语义

## Spark Streaming的容错机制能够保证“exactly once”的容错语义

- 仅针对流计算系统引擎本身而言的
- 一个完整的流计算处理流程不仅涉及流计算系统本身，还涉及提供数据源和接收处理结果的系统，即端到端的过程
- 假设提供数据源的系统无法支持数据的重放，或接收处理结果系统并不支持对已有数据的撤销或修改，那么端到端的容错也无法保证“exactly once”

22

## Page 23

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.5.2 Structured Streaming

- Spark Structured Streaming是全新的流处理引擎
- 是基于Spark SQL引擎扩展的流处理引擎
- 致力于实现“批流一体化”
- 用户编写流处理程序的首选

23

## Page 24

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.5.2 Structured Streaming

## Structured Streaming的特点

- 模型简洁
- 将不断流入的数据抽象为一张“无界表”

- 具有一致的API
- Structured Streaming的SQL操作和Spark SQL共用大部分API，因此使用Spark SQL的进行离线计算的任务可以很快转移到流式计算上

- 性能卓越
- 会直接使用Spark SQL的Catalyst优化器和Tungsten，数据处理性能十分出色

- 支持多种语言
- 支持Scala、Java、Python和R语言

24

## Page 25

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.5.2 Structured Streaming

## 数据模型

- 将持续不断的数据流看作一张不断追加行的无界表，这样流计算被抽象为基于增量数据表的SQL操作
- Structured Streaming在有增量数据时将增量数据收集起来并组成微批数据，然后在微批数据上执行用户定义的SQL操作以完成流计算

![img-3.jpeg](img-3.jpeg)

Data stream as an unbounded table

## Page 26

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.5.2 Structured Streaming

## 数据模型

- Structured Streaming会产生一张结果表
- 查询输出结果有3种不同模式
- 完全模式
- 追加模式
- 更新模式

![img-4.jpeg](img-4.jpeg)
Programming Model for Structured Streaming

26

## Page 27

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.5.2 Structured Streaming

## 基本操作

- 可以对Structured Streaming执行各种基本操作
- SQL操作（如select、where、groupBy等）
- RDD类操作（如map、filter、flatMap等）

27

## Page 28

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.5.2 Structured Streaming

## Structured Streaming程序示例

```java
import org.apache.spark.;
import org.apache.spark.sql.;
import org.apache.spark.sql.functions.;
import org.apache.spark.streaming.;
import org.apache.kafka.clients.;
import org.apache.spark.sql.kafka010.;
```

```java
object Test

def main(args: Array[String]): Unit = {
val spark = SparkSession.builder()
.appName("demo")
.master("local")
.getOrCreate();
spark.sparkContext.setLogLevel("ERROR");
import spark.implicits.;
```

```java
val df = spark.readStream.format("kafka")
.option("kafka.bootstrap.servers", "localhost:9092")
.option("subscribe", "topictest")
.load();
val wordCounts = df.selectExpr("CAST(value as STRING)")
.as[String]
.flatMap(_.split(""))
.groupBy("value")
.count();
val query = wordCounts.writeStream
.outputMode("complete")
.format("console")
.start();
query.awaitTermination();
}
```

28

## Page 29

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.5.2 Structured Streaming

## 时间概念

- 事件时间(event-time)：数据采集的时间
- 通常是数据中的一列，可以用于聚合计算
- 摄取时间(ingestion-time)：数据进入系统的时间
- 处理时间(window-processing-time)：数据进入计算节点并开始出处理的时间，也就是数据触发窗口计算的时间

![img-5.jpeg](img-5.jpeg)

## Page 30

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.5.2 Structured Streaming

## 时间操作操作

- Structured Streaming提供了基于滑动窗口的事件时间聚合操作
- 过程与分组聚合类似
- 按照窗口时间对窗口内的数据执行聚合操作

30

## Page 31

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.5.2 Structured Streaming

## 时间窗口操作

- Structured Streaming的滑动窗口主要包括
- 窗口时间：窗口的大小，用于确定数据操作的长度
- 滑动步长：窗口每次向前移动的时间长度
- 触发时间：Structured Streaming将数据写入外部DataStreamWriter的时间间隔

![img-6.jpeg](img-6.jpeg)

## Page 32

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.5.2 Structured Streaming

## 时间窗口操作示例代码

```java
object Test {
def main(args: Array[String]): Unit = {
val spark = SparkSession.builder()
.appName("demo")
.master("local")
.getOrCreate();
spark.sparkContext.setLogLevel("ERROR");
import spark.implicits.;
val df = spark.readStream.format("kafka")
.action("kafka.bootstrap.servers", "localhost:9092")
.action("subscribe", "topictest")
.load();
val wordCounts = df.selectExpr("timestamp", "CAST(value AS STRING)")
.withColumn("word", functions.explode(functions.split(functions.col("value")," ")))
.groupBy(
window($"timestamp", "10 seconds", "5 seconds"), $"word")
.count();
val query = wordCounts.writeStream
.outputMode("update")
.format("console")
.start();
query.awaitTermination();
}
}
```

32

## Page 33

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.5.2 Structured Streaming

## 延迟数据处理策略

- 数据延迟指的是先产生的数据后到达，这说明数据并没有严格按照先产生的数据先到达的原则发送到服务器
- Structured Streaming能够处理迟到的数据，允许将延迟到达的数据添加到它们本来应该在的位置

![img-7.jpeg](img-7.jpeg)

## Page 34

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.5.2 Structured Streaming

## ❖ 水位线（Watermark）

- 为了保障有足够的内存资源不断处理到来的数据，Spark必须限制积累到内存中的中间状态的数据量
- Spark通过引入水位线，可以自动跟踪数据中的当前事件时间，并尝试清除相应的状态数据
- 可通过指定事件时间列定义查询的水位线，并根据事件时间预测数据的延迟时间。
- 不晚于阈值的滞后数据将被聚合，但晚于阈值的数据将被丢弃
- 水位线的使用可以调用withWatermark()方法来实现

34

## Page 35

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

8.5.2 Structured Streaming

# 水位线代码示例

```java
object Test {
def main(args: Array[String]): Unit = {
val spark = SparkSession.builder()
.appName("demo")
.master("local")
.getOrCreate();
spark.sparkContext.setLogLevel("ERROR");
import spark.implicits.;
val df = spark.readStream.format("kafka")
.action("kafka.bootstrap.servers", "localhost:9092")
.action("subscribe", "topictest")
.load();
val wordCounts = df.selectExpr("timestamp", "CAST(value AS STRING)")
.withColumn("word", functions.explode(functions.split(functions.col("value")," ")))
.withWatermark("timestamp","10 seconds")
.groupBy(
window($"timestamp", "10 seconds", "5 seconds"), $"word")
.count();
val query = wordCounts.writeStream
.outputMode("update")
.format("console")
.start();
query.awaitTermination();
}
}
```

35

## Page 36

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.5.2 Structured Streaming

## 输出模式

- 查询输出结果有3种不同模式
- 追加模式（默认）
- 当确定不会更新时间窗口时，就输出时间中的数据并删除，以保证每个时间窗口内的数据只输出一次
- 完全模式
- 不删除任何数据，而是在结果表中保留所有数据，每次触发聚合操作时，就输出所有时间窗口中的数据
- 更新模式
- 删除不再更新的时间窗口，每次触发聚合操作时，输出更新的时间窗口

36

## Page 37

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.5.2 Structured Streaming

## 使用水位线清除聚合状态的条件

- Structured Streaming支持通过水位线定期清除中间状态数据
- 需满足以下条件：
- 输出模式必须是追加或更新模式
- 完全模式由于要求保留所有聚合数据，因此不能使用水位线中断状态
- 聚合中必须包含事件时间（event-time）列或窗口事件时间列
- withWaterMark()方法使用的时间戳必须与聚合使用的时间戳列相同
- withWaterMark()方法必须在聚合之前调用

37

## Page 38

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.5.2 Structured Streaming

## 触发器(trigger)

- 流式查询的触发器定义了流式数据处理的时间
- Structured Streaming支持基于4种策略的触发器
- 未指定（默认）
- 查询在微批模式下执行。上一次微批处理完成后，就尽可能快执行下一个微批处理
- 固定时间微批
- 查询以用户指定的时间间隔启动
» 上一个微批在指定的时间内完成，Spark将等待间隔结束后启动下一个微批；上一个微批没有指定的时间内完成，那么下一个微批将在上一个微批完成后立即开始，这种情况下可能存在批处理延迟；如果没有新数据可用，则不会启动微批处理
- 一次微批执行
- 只执行一个微批的所有可用数据，然后自行停止
- 当前可用的微批处理
- 处理所有可用数据后自动停止。不同的是会根据数据源配置选项将数据划分为（可能的）多个微批次进行处理，从而显著提升查询的可扩展性
- 连续的固定检查点间隔（实验阶段）
- 将连续以异步方式处理微批数据

38

## Page 39

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.5.2 Structured Streaming

## 触发器示例

- 每5s启动一次微批处理

```java
object Test {
def main(args: Array[String]): Unit = {
val spark = SparkSession.builder()
.appName("demo")
.master("local")
.getOrCreate();
spark.sparkContext.setLogLevel("ERROR");
import spark.implicits.;
val df = spark.readStream.format("kafka")
.action("kafka.bootstrap.servers", "localhost:9092")
.action("subscribe", "topictest")
.load();
val wordCounts = df.selectExpr("timestamp", "CAST(value AS STRING)")
.withColumn("word", functions.explode(functions.split(functions.col("value"), " ")))
.withWatermark("timestamp", "10 seconds")
.groupBy(
window($"timestamp", "10 seconds", "5 seconds"), $"word"
.count();
val query = wordCounts.writeStream
.trigger(Trigger.ProcessingTime(5, TimeUnit.SECONDS))
.outputMode("update")
.format("console")
.start();
query.awaitTermination();
}
}
```

39

## Page 40

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.5.2 Structured Streaming

## 有状态流处理

- mapGroupsWithState
- flatMapGroupsWithState

## 注意

- 仅适用于KeyValueGroupedDataset（可以groupByKey生成）
- 超时配置：支持处理时间（ProcessingTimeTimeout）和事件时间（EventTimeTimeout）两种超时模式。若使用事件时间超时，需启用水印（withWatermark）
- mapGroupsWithState每个分组必须返回单个输出记录，flatMapGroupsWithState允许返回零个、一个或多个输出记录

40

## Page 41

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.5.2 Structured Streaming

## 有状态流处理

```java
// A mapping function that maintains an integer state for string keys and returns a string.
// Additionally, it sets a timeout to remove the state if it has not received data for an hour.
def mappingFunction(key: String, value: Iterator[Int], state: GroupState[Int]): String = {
// If (state.hasTimedOut) {
// If called when timing out, remove the state
state.remove()

} else if (state.exists) {
// If state exists, use it for processing
val existingState = state.get
// Get the existing state
val shouldRemove = ...
// Decide whether to remove the state
if (shouldRemove) {
state.remove()
// Remove the state

} else {
val newState = ...
state.update(newState)
// Set the new state
state.setTimeoutDuration("1 hour") // Set the timeout
}

} else {
val initialState = ...
state.update(initialState)
// Set the initial state
state.setTimeoutDuration("1 hour") // Set the timeout
}
...
// return something
}

dataset
.groupByKey(...)
.mapGroupsWithState(GroupStateTimeout.ProcessingTimeTimeout)(mappingFunction)

## Page 42

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.5.2 Structured Streaming

## exactly once语义的实现

- Structured Streaming提供了端到端的exactly once语义
- 通过结构化数据源、接收器、执行引擎，Structured Streaming能够可靠地跟踪数据处理的确切进展，从而保证在故障中能够快速恢复计算并确保数据的一致性

42

## Page 43

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.5.2 Structured Streaming

## exactly once语义的实现原理

- 每个数据流都有一个数据的偏移量用来标识当前处理位置
- Structured Streaming通过检查点和预写日志（WAL）记录每个触发器中正在处理的数据偏移范围，并在数据处理完毕后提交偏移量
- Structured Streaming的数据接收器具有幂等性
- 发生故障后，可以利用数据接收器的幂等性，通过数据重放赖确保任何故障下的端到端的exactly once语义

43

## Page 44

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.5.2 Structured Streaming

## exactly once语义的实现原理

![img-8.jpeg](img-8.jpeg)

## Page 45

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.5.2 Structured Streaming

## 结果输出

- Structured Streaming的输出操作支持

![img-9.jpeg](img-9.jpeg)

## Page 46

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.5.2 Structured Streaming

## 结果输出

- 不同输出支持的输出模式和容错性

|  输出操作 | 支持的输出模式 | 容错性支持  |
| --- | --- | --- |
|  File Sink | 追加模式 | 支持（exactly-once，准确一性）  |
|  Kafka Sink | 追加模式、更新模式、完全模式 | 支持（at-least-once，至少一次）  |
|  foreach Sink | 追加模式、更新模式、完全模式 | 支持（at-least-once，至少一次）  |
|  foreachBatch Sink | 追加模式、更新模式、完全模式 | 依赖于实现逻辑  |
|  Console Sink | 追加模式、更新模式、完全模式 | 不支持  |
|  Memory Sink | 追加模式、完全模式 | 不支持（但在完全模式下重启查询时会重建整个内存表）  |

46

## Page 47

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 主要内容

8.1 Spark概述
8.2 Spark基础
8.3 Spark RDD
8.4 Spark SQL
8.5 Spark流计算
8.6 Spark机器学习

47

## Page 48

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.6 Spark机器学习

8.6.1 分布式机器学习简介
8.6.2 Spark MLlib简介
8.6.3 ML Pipeline
8.6.4 具体示例

48

## Page 49

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.6.1 分布式机器学习

## 什么是机器学习

### 几种定义

- 机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，特别是如何在经验学习中改善具体算法的性能。
- 机器学习是对能通过经验自动改进的计算机算法的研究。
- 机器学习是用数据或以往的经验，以此优化计算机程序的性能标准。

- T:Task
- E:Experience
- P:Performance

![img-10.jpeg](img-10.jpeg)

## Page 50

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.6.1 分布式机器学习

## 什么是分布式机器学习

### 背景

- 大数据：大量有标签的训练数据
- ImageNet数据集，包含2万多个类别、上千万张图片，大小约150G
- 大模型：模型参数巨大
- GPT-3：约1750亿个参数，训练数据达45TB
- 带来的问题：
- 单机资源已经无法满足大数据时代机器学习的需要

50

## Page 51

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.6.1 分布式机器学习

## 什么是分布式机器学习

- 分布式机器学习：通过庞大的计算机集群（尤其是GPU集群），在大规模的数据集上并行训练出更加准确的大规模机器学习模型（大模型）。
- 系统主要构成模块：
- 数据和模型划分模块
- 单机优化模块
- 通信模块
- 模型和数据聚合模块
- 主要研究问题：
- 如何分配训练任务
- 如何调配资源
- 如何协调各大模块

最终达到训练速度和精度的平衡。

51

## Page 52

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

52

# 8.6.1 分布式机器学习

## 分布式机器学习

- 并行策略
- 数据并行
- 随机采样
- 置乱切分
- 模型并行
- 横向按层划分
- 纵向跨层划分
- 模型随机并行
- 混合策略

## Page 53

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

53

# 8.6.1 分布式机器学习

## 分布式机器学习

- 通信架构
- 迭代式MapReduce/All Reduce
- 参数服务器
- 数据流

## Page 54

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.6.1 分布式机器学习

## 分布式机器学习

- 通信架构
- 迭代式MapReduce/All Reduce
- 典型系统：Spark MLlib、SystemML、REEF

![img-11.jpeg](img-11.jpeg)

## Page 55

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.6.1 分布式机器学习

## 分布式机器学习

- 通信架构
- 参数服务器
- 典型框架：CMU Parameter Server、DistBelief、Multiverso
- 典型系统：MxNet、Paddle、DMTK、Petumm

![img-12.jpeg](img-12.jpeg)

## Page 56

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.6.1 分布式机器学习

## 分布式机器学习

- 通信架构
- 数据流
- 典型系统：TensorFlow

![img-13.jpeg](img-13.jpeg)

## Page 57

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.6.2 Spark MLlib简介

- Spark提供了一个基于海量数据的机器学习库，它提供了常用机器学习算法的分布式实现
- 开发者只需要有 Spark 基础并且了解机器学习算法的原理，以及方法相关参数的含义，就可以轻松的通过调用相应的 API 来实现基于海量数据的机器学习过程
- Spark-Shell的即席查询也是一个关键。算法工程师可以边写代码边运行，边看结果

57

## Page 58

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.6.2 Spark MLlib简介

- MLlib是Spark的机器学习（Machine Learning）库，旨在简化分布式机器学习的工程实践工作
- MLlib由一些通用的学习算法和工具组成，包括分类、回归、聚类、协同过滤、降维等，同时还包括底层的优化原语和高层的流水线（Pipeline）API

58

## Page 59

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

8.6.2 Spark MLlib简介

|  分类&回归 |   | 推荐 | 算法评测  |
| --- | --- | --- | --- |
|  线性模型 | 朴素贝叶斯 | ALS | AUC  |
|  线性变种向量机SVMs(分类) | 决策树 | 关联规则 |   |
|  逻辑回归(分类) | RF&GBDT | Fp-growth | 准确率  |
|  线性回归 |  |  |   |
|  聚类 | 陡维 | 优化 | 百回率  |
|  K-means | SVD | 随机标准下降 |   |
|  LDA | PCA | L-BFGS | F-measure  |
|  特征抽取 |   | 统计 |   |
|  TF-IDF | StandardScaler | 旧关性 |   |
|  Word2Vec | Normalizer | 分层抽样 |   |
|   | ChiSqSelector | 假设检验 |   |

59

## Page 60

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.6.2 Spark MLlib简介

## Spark MLlib分为两个包

- spark.mllib: 包含基于RDD的原始算法API。Spark MLlib 历史比较长，在1.0以前的版本即已经包含了，提供的算法实现都是基于原始的RDD。Spark 2.0开始进入维护模式。
- spark.ml: 则提供了基于DataFrames 高层次的API，可以用来构建机器学习工作流（PipeLine）。ML Pipeline 弥补了原始 MLlib 库的不足，向用户提供了一个基于 DataFrame 的机器学习工作流式 API 套件

60

## Page 61

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

8.6.3 ML Pipeline

# 8.6.3 ML Pipeline

- 八个重要概念
- DataFrame
- Transformer
- Estimator
- Pipeline
- Parameter

61

## Page 62

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

8.6.3 ML Pipeline

## DataFrame

- 使用Spark SQL中的DataFrame作为数据集，它可以容纳各种数据类型。
- 较之RDD，DataFrame包含了schema 信息，更类似传统数据库中的二维表格。
- 它被ML Pipeline用来存储源数据。例如，DataFrame中的列可以是存储的文本、特征向量、真实标签和预测的标签等

62

## Page 63

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.6.3 ML Pipeline

## Transformer

- 是一种可以将一个DataFrame转化为另一个DataFrame的算法。比如一个模型就是一个Transformer。它可以把一个不包含预测标签的测试数据集 DataFrame 打上标签，转化成另一个包含预测标签的 DataFrame。
- Transformer实现了一个方法transform()，它通过附加一个或多个列将一个DataFrame转化为另一个DataFrame

63

## Page 64

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.6.3 ML Pipeline

## Estimator

- 是学习算法或在训练数据上的训练方法的概念抽象。在 Pipeline 里通常是被用来操作 DataFrame 数据并生成一个 Transformer。
- Estimator 实现了一个方法 fit()，它接受一个 DataFrame 并产生一个 Transformer。
- 比如，一个随机森林算法就是一个 Estimator，它可以调用 fit()，通过训练特征数据而得到一个随机森林模型。

64

## Page 65

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

8.6.3 ML Pipeline

- Pipeline
- 将多个工作流阶段（Transformer和Estimator）连接在一起，形成机器学习的工作流，并获得结果输出。

65

## Page 66

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

8.6.3 ML Pipeline

## Parameter

- Parameter 被用来设置 Transformer 或者 Estimator 的参数。所有 Transformer 和 Estimator 可共享用于指定参数的公共 API。ParamMap 是一组（参数，值）对

66

## Page 67

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.6.3 ML Pipeline

要构建一个 Pipeline，首先需要定义 Pipeline 中的各个 PipelineStage，按照具体的处理逻辑有序地组织 PipelineStages 并创建一个 Pipeline

```bash
val pipeline = new Pipeline().setStages(Array(stage1, stage2, stage3, ...))
```

然后就可以把训练数据集作为输入参数，调用 Pipeline 实例的 fit 方法来开始以流的方式来处理源训练数据。这个调用会返回一个 PipelineModel 类实例，进而被用来预测测试数据的标签

67

## Page 68

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.6.3 ML Pipeline

- Pipeline的各个阶段按顺序运行，输入的DataFrame在它通过每个阶段时被转化

![img-14.jpeg](img-14.jpeg)

## Page 69

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.6.3 ML Pipeline

- Pipeline本身也可以看做是一个Estimator，在运行fit()方法之后，它产生一个PipelineModel，它是一个Transformer，可以在测试数据的时候使用。

![img-15.jpeg](img-15.jpeg)

## Page 70

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

8.6.4 具体示例

以Logistic Regression为例，构建一个典型的机器学习过程

- 查找出所有包含"spark"的句子，即将包含"spark"的句子的标签设为1，没有"spark"的句子的标签设为0。

70

## Page 71

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

8.6.4 具体示例

## 具体步骤

- (1) 引入必要包，构建SparkSession对象

```java
package sparkdemo.ml;

import org.apache.spark.;
import org.apache.spark.sql.;
import org.apache.spark.ml.;
import org.apache.spark.ml.classification.;
import org.apache.spark.ml.feature.;
import org.apache.spark.ml.linalg.;
```

```java
object Test {
def main(args: Array[String]): Unit = {
val spark = SparkSession.builder()
.appName("mldemo")
.master("local")
.getOrCreate();
}

## Page 72

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

72

# 8.6.4 具体示例

## 具体步骤

- (2) 构建训练数据集

```c
val training = spark.createDataFrame(Seq(
(0L, "a b c d e spark", 1.0),
(1L, "b d", 0.0),
(2L, "spark f g h", 1.0),
(3L, "hadoop mapreduce", 0.0))).toDF("id", "text", "label");
```

## Page 73

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

8.6.4 具体示例

## 具体步骤

### （3）定义Pipeline

```c
val tokenizer = new Tokenizer()
.setInputCol("text")
.setOutputCol("words");
val hashingTF = new HashingTF()
.setNumFeatures(1000)
.setInputCol(tokenizer.getOutputCol)
.setOutputCol("features")
val lr = new LogisticRegression()
.setMaxIter(10)
.setRegParam(0.001)
val pipeline = new Pipeline()
.setStages(Array(tokenizer, hashingTF, lr));
```

73

## Page 74

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

74

# 8.6.4 具体示例

## 具体步骤

- (4) 训练模型

```javascript
val model = pipeline.fit(training);
```

## Page 75

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

75

# 8.6.4 具体示例

## 具体步骤

- (5) 构建测试数据集

```c
val test = spark.createDataFrame(Seq(
(4L, "spark i j k"),
(5L, "l m n"),
(6L, "spark hadoop spark"),
(7L, "apache hadoop")).toDF("id", "text");
```

## Page 76

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

8.6.4 具体示例

## 具体步骤

- （6）对测试数据进行预测，并输出结果

```javascript
val pred = model.transform(test);
pred.show();
}
```

|  id | text | words | features | rawPrediction | probability | prediction  |
| --- | --- | --- | --- | --- | --- | --- |
|  4 | spark i j k | [spark, i, j, k] | (1000, [286,541,66...] [0.52882855227967...] [0.62920984896684...] [0.0] | 0.0 |  |   |
|  5 | l m n | [l, m, n] | (1000, [490,575,74...] [4.16914139534005...] [0.98477000676230...] [0.0] | 0.0 |  |   |
|  6 | spark hadoop spark | [spark, hadoop, s...] | (1000, [286,585], [...][-1.8649814141189...] [0.13412348342566...] [1.0] | 1.0 |  |   |
|  7 | apache hadoop | [apache, hadoop] | (1000, [583,585], [...][5.41564427200185...] [0.99557321143985...] [0.0] | 0.0 |  |   |

## Page 77

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

谢谢！

---

## Extracted Images

Total images found: 16

### Image 1

- **ID**: `img-0.jpeg`
- **Position**: (312, 768) to (1626, 970)
- **Size**: 1314 × 202 pixels

### Image 2

- **ID**: `img-1.jpeg`
- **Position**: (328, 855) to (1594, 1423)
- **Size**: 1266 × 568 pixels

### Image 3

- **ID**: `img-2.jpeg`
- **Position**: (202, 652) to (1648, 1191)
- **Size**: 1446 × 539 pixels

### Image 4

- **ID**: `img-3.jpeg`
- **Position**: (394, 826) to (1596, 1312)
- **Size**: 1202 × 486 pixels

### Image 5

- **ID**: `img-4.jpeg`
- **Position**: (838, 572) to (1750, 1302)
- **Size**: 912 × 730 pixels

### Image 6

- **ID**: `img-5.jpeg`
- **Position**: (586, 887) to (1678, 1389)
- **Size**: 1092 × 502 pixels

### Image 7

- **ID**: `img-6.jpeg`
- **Position**: (332, 805) to (1694, 1411)
- **Size**: 1362 × 606 pixels

### Image 8

- **ID**: `img-7.jpeg`
- **Position**: (334, 712) to (1716, 1330)
- **Size**: 1382 × 618 pixels

### Image 9

- **ID**: `img-8.jpeg`
- **Position**: (238, 477) to (1646, 1159)
- **Size**: 1408 × 682 pixels

### Image 10

- **ID**: `img-9.jpeg`
- **Position**: (172, 556) to (1696, 1117)
- **Size**: 1524 × 561 pixels

### Image 11

- **ID**: `img-10.jpeg`
- **Position**: (758, 919) to (1630, 1336)
- **Size**: 872 × 417 pixels

### Image 12

- **ID**: `img-11.jpeg`
- **Position**: (396, 681) to (1198, 1342)
- **Size**: 802 × 661 pixels

### Image 13

- **ID**: `img-12.jpeg`
- **Position**: (554, 745) to (1346, 1318)
- **Size**: 792 × 573 pixels

### Image 14

- **ID**: `img-13.jpeg`
- **Position**: (334, 711) to (1668, 1036)
- **Size**: 1334 × 325 pixels

### Image 15

- **ID**: `img-14.jpeg`
- **Position**: (154, 522) to (1766, 949)
- **Size**: 1612 × 427 pixels

### Image 16

- **ID**: `img-15.jpeg`
- **Position**: (114, 750) to (1802, 1246)
- **Size**: 1688 × 496 pixels

---

*Generated by Mistral OCR MCP Server*
