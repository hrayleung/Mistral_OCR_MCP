---
source: /Users/hinrayleung/Documents/BNU/dataAna/PPT/第8章+大数据处理平台Spark+-+第1部分.pdf
type: pdf
model: mistral-ocr-latest
processed: 2026-01-03T22:55:44.144920
---

# Document: 第8章+大数据处理平台Spark+-+第1部分.pdf

## Table of Contents

- [Page 1](#page-1)
- [Page 2](#page-2)
- [Page 3](#page-3)
- [Page 4](#page-4)
- [Page 5](#page-5)
- [Page 6](#page-6)
- [Page 7](#page-7)
- [Page 8](#page-8)
- [Page 9](#page-9)
- [Page 10](#page-10)
- [Page 11](#page-11)
- [Page 12](#page-12)
- [Page 13](#page-13)
- [Page 14](#page-14)
- [Page 15](#page-15)
- [Page 16](#page-16)
- [Page 17](#page-17)
- [Page 18](#page-18)
- [Page 19](#page-19)
- [Page 20](#page-20)
- [Page 21](#page-21)
- [Page 22](#page-22)
- [Page 23](#page-23)
- [Page 24](#page-24)
- [Page 25](#page-25)
- [Page 26](#page-26)
- [Page 27](#page-27)
- [Page 28](#page-28)
- [Page 29](#page-29)
- [Page 30](#page-30)
- [Page 31](#page-31)
- [Page 32](#page-32)
- [Page 33](#page-33)
- [Page 34](#page-34)
- [Page 35](#page-35)
- [Page 36](#page-36)
- [Page 37](#page-37)
- [Page 38](#page-38)
- [Page 39](#page-39)
- [Page 40](#page-40)
- [Page 41](#page-41)
- [Page 42](#page-42)
- [Page 43](#page-43)
- [Page 44](#page-44)
- [Page 45](#page-45)
- [Page 46](#page-46)
- [Page 47](#page-47)
- [Page 48](#page-48)
- [Page 49](#page-49)
- [Page 50](#page-50)
- [Page 51](#page-51)
- [Page 52](#page-52)
- [Page 53](#page-53)
- [Page 54](#page-54)
- [Page 55](#page-55)
- [Page 56](#page-56)
- [Page 57](#page-57)
- [Page 58](#page-58)
- [Page 59](#page-59)
- [Page 60](#page-60)
- [Page 61](#page-61)
- [Page 62](#page-62)
- [Page 63](#page-63)
- [Page 64](#page-64)
- [Page 65](#page-65)
- [Page 66](#page-66)
- [Page 67](#page-67)
- [Page 68](#page-68)
- [Page 69](#page-69)
- [Page 70](#page-70)
- [Page 71](#page-71)
- [Page 72](#page-72)
- [Page 73](#page-73)
- [Page 74](#page-74)
- [Page 75](#page-75)
- [Page 76](#page-76)
- [Page 77](#page-77)
- [Page 78](#page-78)
- [Page 79](#page-79)
- [Page 80](#page-80)
- [Page 81](#page-81)
- [Page 82](#page-82)
- [Page 83](#page-83)
- [Page 84](#page-84)
- [Page 85](#page-85)
- [Page 86](#page-86)
- [Page 87](#page-87)
- [Page 88](#page-88)
- [Page 89](#page-89)
- [Page 90](#page-90)
- [Page 91](#page-91)
- [Page 92](#page-92)
- [Page 93](#page-93)
- [Page 94](#page-94)
- [Page 95](#page-95)

## Page 1

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 第8章 大数据处理平台Spark

## Page 2

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 主要内容

8.1 Spark概述
8.2 Spark基础
8.3 Spark RDD
8.4 Spark SQL
8.5 Spark流计算
8.6 Spark机器学习

2

## Page 3

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.1 Spark概述

- MapReduce在大数据处理方面取得了巨大成功，但其本身存在一定局限性。
- 表达能力有限
- 仅提供map和reduce两个算子
- 磁盘I/O开销大
- 每次执行都需要从磁盘读取数据，中间结果需写入磁盘
- 延迟高
- 多个作业之间按顺序执行

## Page 4

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.1 Spark概述

- Spark诞生于UC Berkeley的AMP（Algorithms, Machine and People）实验室，是基于内存计算的大数据并行计算框架，可用于构建大型的、低延迟的数据分析应用程序
- 2009年，AMP实验室开发出Spark
- 2010年，Spark正式开源
- 2013年，Spark成为Apache开源项目
- 2014年，Spark成为Apache顶级项目
- 2013年，Databricks公司成立（Spark多位创始人联合创立），致力于Spark商业化

4

## Page 5

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.1 Spark概述

## Spark具有如下几个主要特点：

- 运行速度快：使用DAG执行引擎以支持循环数据流与内存计算
- 容易使用：支持使用Scala、Java、Python和R语言进行编程，可以通过Spark Shell进行交互式编程
- 通用性：Spark提供了完整而强大的技术栈，包括SQL查询、流式计算、机器学习和图计算组件
- 运行模式多样：可运行于独立的集群模式中，可运行于Hadoop中，也可运行于Amazon EC2等云环境中，并且可以访问HDFS、Cassandra、HBase、Hive等多种数据源

## Page 6

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.1 Spark概述

与Hadoop MapReduce相比，Spark主要具有如下优点：

- 编程模型比Hadoop MapReduce更灵活
- Spark的计算模式也属于MapReduce，但不局限于Map和Reduce操作，提供了丰富的操作
- Spark提供了内存计算，可将中间结果放到内存中，对于迭代运算效率更高
- Spark基于DAG的任务调度执行机制，要优于Hadoop MapReduce的迭代执行机制

6

## Page 7

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

8.1 Spark概述

## Hadoop与Spark的执行流程对比

![img-0.jpeg](img-0.jpeg)
(a) Hadoop MapReduce执行流程

![img-1.jpeg](img-1.jpeg)

![img-2.jpeg](img-2.jpeg)
(b) Spark执行流程

## Page 8

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.1 Spark概述

## Spark主要模块

![img-3.jpeg](img-3.jpeg)

## Page 9

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.1 Spark概述

## Spark主要模块

- Spark Core: Spark核心模块
- Spark SQL: 结构化数据处理
- Spark Straming: 流处理（构建在Spark RDD上）
- Structured Straming: 流处理（构建在Spark SQL上）
- MLlib: 机器学习
- GraphX: 图计算

## Page 10

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 主要内容

8.1 Spark概述
8.2 Spark基础
8.3 Spark RDD
8.4 Spark SQL
8.5 Spark流计算
8.6 Spark机器学习

10

## Page 11

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.2 Spark基础

- 8.2.1 基本概念
- 8.2.2 运行架构
- 8.2.3 Spark Shell

11

## Page 12

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.2.1 基本概念

## 基本概念

- RDD：是Resilient Distributed Dataset（弹性分布式数据集）的简称，是分布式内存的一个抽象概念，提供了一种高度受限的共享内存模型
- DAG：是Directed Acyclic Graph（有向无环图）的简称，反映RDD之间的依赖关系
- Executor：是运行在工作节点（Worker Node）的一个进程，负责运行Task
- 应用（Application）：用户编写的Spark应用程序
- 作业（Job）：一个作业包含多个RDD及作用于相应RDD上的各种操作
- 阶段（Stage）：是作业的基本调度单位，一个作业会分为多组任务，每组任务被称为阶段，或者也被称为任务集合，代表了一组关联的、相互之间没有Shuffle依赖关系的任务组成的任务集
- 任务（Task）：运行在Executor上的工作单元

12

## Page 13

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.2.2 运行架构

## Spark运行模式

- Spark 有多种运行模式
- local：本地模式，又分为local单线程和local-cluster多线程
- Standalone：独立集群模式，运行在自带的资源管理器上
- on Hadoop YARN：运行在YARN上
- on Kubernetes：运行在Kubernetes上

## Page 14

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.2.2 运行架构

## Spark运行架构包括
- 集群资源管理器（Cluster Manager）
- 工作节点（Worker Node）
- 执行器（Executor）
- 驱动器（Driver Program）

![img-4.jpeg](img-4.jpeg)

## Page 15

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

8.2.2 运行架构

- 驱动器（Driver Program）
- 包含运行应用程序的主函数和构建SparkContext实例的程序
- 驱动器可以运行在应用程序的节点上，也可以由应用程序提交给集群管理器，再由集群管理器安排给工作节点运行

15

## Page 16

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.2.2 运行架构

## SparkContext

- 是Spark应用程序的入口，是应用程序和Spark集群交互的通道
- 主要用于初始化Spark应用程序所需的基础组件，主要包括：
- DAG调度器：高层调度器，负责DAG层面的调度，主要用于构建DAG、划分Stage以及提交Stage到任务调度器
- 任务调度器：底层调度器，负责任务层面的调度，主要负责接收DAG调度器发送过来的任务集以及将任务集加载并注册到集群管理器
- SchedulerBackend：是调度器的通信终端，主要负责运行任务所需要资源的申请
- SparkContext还负责向Spark管理节点注册应用程序等

## Page 17

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.2.2 运行架构

## Standalone模式
- 采用经典的主从架构
- 采用自带的独立调度器(Standalone)

![img-5.jpeg](img-5.jpeg)
clinet运行方式

![img-6.jpeg](img-6.jpeg)
cluster运行方式

## Page 18

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.2.2 运行架构

## Spark On YARN模式
- YARN负责资源管理
- Spark负责任务调度和计算

![img-7.jpeg](img-7.jpeg)
clinet运行方式

![img-8.jpeg](img-8.jpeg)
cluster运行方式

## Page 19

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.2.2 运行架构

一个应用由一个Driver和若干个作业构成，一个作业由多个阶段构成，一个阶段由多个没有Shuffle关系的任务组成

![img-9.jpeg](img-9.jpeg)

## Page 20

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.2.2 运行架构

## Spark运行基本流程

![img-10.jpeg](img-10.jpeg)

## Page 21

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.2.2 运行架构

## Spark运行基本流程

- （1）首先为应用构建起基本的运行环境，即由Driver创建一个SparkContext，进行资源的申请、任务的分配和监控
- （2）资源管理器为Executor分配资源，并启动Executor进程
- （3）SparkContext根据RDD的依赖关系构建DAG图，DAG图提交给DAGScheduler解析成Stage，然后把一个个TaskSet提交给底层调度器TaskScheduler处理；Executor向SparkContext申请Task，Task Scheduler将Task发放给Executor运行，并提供应用程序代码
- （4）Task在Executor上运行，把执行结果反馈给TaskScheduler，然后反馈给DAGScheduler，运行完毕后写入数据并释放所有资源

## Page 22

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

8.2.2 运行架构

提交应用程序
- 启动HDFS集群
- 启动YARN集群
- 以Spark on YARN的cluster模式运行示例

```shell
hadoop@tempo-talents:/usr/local/spark$ spark-submit \
&gt; --class org.apache.spark.examples.SparkPi \
&gt; --master yarn \
&gt; --deploy-mode cluster \
&gt; examples/jars/spark-examples_2.12-3.2.3.jar
```

22

## Page 23

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.2.2 运行架构

|  表 16-4 spark-submit 的常用参数介绍  |   |
| --- | --- |
|  参数 | 描述  |
|  --master | Master 节点的连接地址。取值为 spark://host:port、mesos://host:port、yam、k8s://https://host:port 或 local (默认为 local[*])  |
|  --deploy-mode | 运行方式。取值为 “client” 或 “cluster”。“client” 表示在本地客户端启动 Driver 程序，“cluster” 表示在集群内部的工作节点上启动 Driver 程序。默认为 “client”  |
|  --class | 应用程序的主类（Java 或 Scala 程序）  |
|  --name | 应用程序名称，会在 Spark Web UI 中显示  |
|  --jars | 应用依赖的第三方的 jar 包列表，以逗号分隔  |
|  --files | 需要放到应用工作目录中的文件列表，以逗号分隔。此参数一般用来放需要分发到各节点的数据文件  |
|  --conf | 设置任意的 SparkConf 配置属性，格式为“属性名=属性值”  |
|  --properties-file | 加载外部包含键值对的属性文件。如果不指定，则默认读取 Spark 安装目录下的 conf/spark-defaults.conf 文件中的配置  |
|  --driver-memory | Driver 进程使用的内存量。例如“512 M”或“1 G”，单位不区分大小写。默认为 1024 MB  |
|  --executor-memory | 每个 Executor 进程所使用的内存量。例如“512M”或“1G”，单位不区分大小写。默认为 1 GB  |
|  --driver-cores | Driver 进程使用的 CPU 核心数，仅在集群模式中使用。默认为 1  |
|  --executor-cores | 每个 Executor 进程所使用的 CPU 核心数，默认为 1  |
|  --num-executors | Executor 进程数量，默认为 2。如果开启动态分配，则初始 Executor 的数量至少是此参数配置的数量。  |
|   | 需要注意的是，此参数仅在 Spark On YARN 模式中使用  |

23

## Page 24

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.2.3 Spark Shell

- Spark Shell 提供了简单的方式来学习 Spark API
- Spark Shell 可以以实时、交互的方式来分析数据
- Spark Shell 支持 Scala 和 Python

Spark Shell 本身就是一个 Driver，里面已经包含了 main 方法

24

## Page 25

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.2.3 Spark Shell

## Spark on YARN模式下Spark Shell的启动

bin/spark-shell --master yarn

![img-11.jpeg](img-11.jpeg)

## Page 26

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.2.3 Spark Shell

可以在里面输入scala代码进行调试：

```
scala&gt; 8*2+5
res0: Int = 21
```

可以使用命令“:quit”退出Spark Shell：

```
scala&gt;:quit
```

或者，也可以直接使用“Ctrl+D”组合键，退出Spark Shell

## Page 27

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

8.2.3 Spark Shell

# PySpark on YARN模式下的启动

bin/pyspark --master yarn

![img-12.jpeg](img-12.jpeg)

## Page 28

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 主要内容

8.1 Spark概述
8.2 Spark基础
8.3 Spark RDD
8.4 Spark SQL
8.5 Spark流计算
8.6 Spark机器学习

28

## Page 29

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.3 Spark RDD

- 8.3.1 RDD简介
- 8.3.2 工作原理
- 8.3.3 容错机制
- 8.3.4 编程示例

29

## Page 30

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

8.3.1 RDD简介

## 设计背景

- 许多迭代式算法（比如机器学习、图算法等）和交互式数据挖掘工具，不同计算阶段之间会重用中间结果
- MapReduce计算框架把中间结果写入到稳定存储（比如磁盘）中，带来了大量的数据复制、磁盘IO和序列化开销

30

## Page 31

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.3.1 RDD简介

- RDD是Spark中最基本的数据抽象，代表不可变、可分区和可并行计算的集合
- RDD是对分布式、只读的内存数据的抽象
- RDD支持丰富的操作
- 基于RDD之间的依赖关系可以生成DAG
- RDD可以缓存在内存中，极大提高了RDD的读取效率

31

## Page 32

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

8.3.1 RDD简介

RDD是Spark中最基本的数据抽象，代表不可变、可分区和可并行计算的集合

- RDD是对分布式、只读的内存数据的抽象
- RDD在逻辑上分为多个分区，可以在不同的节点上进行处理，从而提高计算效率
- 分区的个数决定了并行度，每一个分区的数据都在一个单独的任务上执行
- 如果没有指定分区数，Spark会采用默认的分区数（程序运行时分配到的CPU的核心数

32

## Page 33

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.3.1 RDD简介

RDD是Spark中最基本的数据抽象，代表不可变、可分区和可并行计算的集合

- RDD是对分布式、只读的内存数据的抽象
- 要想改变RDD中的数据，只能基于现有的RDD执行转换(transformation)操作生成新的RDD

![img-13.jpeg](img-13.jpeg)

## Page 34

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.3.1 RDD简介

RDD是Spark中最基本的数据抽象，代表不可变、可分区和可并行计算的集合

- RDD支持丰富的操作
- 转换(transformation) 操作：从现有RDD创建新的RDD
- 行动（action）操作：在RDD上进行计算，并将计算结果返回给驱动程序

34

## Page 35

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 常用的转换操作

|  转换操作 | 含义  |
| --- | --- |
|  map(func) | 对 RDD 中的每个记录均使用 func 进行转换，返回一个新的 RDD  |
|  filter(func) | 过滤对 RDD 中的每个记录使用 func 后返回值为 true 的记录  |
|  flatMap(func) | 与 map 类似，但可将 RDD 中的每个记录映射为 0 个或多个新记录  |
|  mapPartitions(func) | 与 map 类似，但对每个分区进行操作  |
|  union(otherRDD) | 两个 RDD 取并集得到一个新的 RDD  |
|  intersect(otherRDD) | 两个 RDD 取交集得到一个新的 RDD  |
|  groupByKey([numPartitions]) | 将[K,V]键值对按键分组，返回一个由[K,Iterable<V>]组成的新的 RDD  |
|  reduceByKey(func, [numPartitions]) | 将键值对按键聚合，在每个键的所有值上使用 func，返回一个由[K,V]组成的新的 RDD  |
|  sortByKey([ascending], [numPartitions]) | 将键值对按键排序，返回一个新的 RDD  |
|  join(otherRDD, [numPartitions]) | [K, V1]和[K, V2]分别属于两个 RDD，返回一个由[K,(V1,V2)]组成的 RDD  |
|  cogroup(otherRDD, [numPartitions]) | [K, V1]和[K, V2]分别属于两个 RDD，返回一个由[K,(Iterable<V1>,Iterable<V2>)]组成的 RDD  |

## Page 36

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 常用的行动操作

|  行动操作 | 含义  |
| --- | --- |
|  reduce(func) | 将 RDD 中的记录按 func 聚合，该 func 必须满足交换律和结合律  |
|  collect() | 将 RDD 中的所有记录收集到 driver 中，返回一个 Array  |
|  count() | 返回 RDD 中记录的个数  |
|  first() | 返回 RDD 中的第 1 个记录  |
|  take(n) | 返回 RDD 中的前 n 个记录  |
|  saveAsTextFile(path) | 将 RDD 中的记录以文本文件的形式写入本地文件系统、HDFS 或其他任何 Hadoop 支持的文件系统中的给定目录  |
|  countByKey() | 按 key 统计计数，返回一个由 [K, Long] 组成的 map  |
|  foreach(func) | 对 RDD 中的每个记录均使用 func  |

## Page 37

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 所有的转换操作都是懒加载的

- 转换操作不会立即执行，而是先记录RDD之间的依赖关系，仅当行动操作出发时才会执行RDD的转换操作并进行计算

![img-14.jpeg](img-14.jpeg)

- 优点：能使Spark更好地对任务进行优化，从而使Spark的运行更加合理、高效。

## Page 38

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

8.3.1 RDD简介

RDD是Spark中最基本的数据抽象，代表不可变、可分区和可并行计算的集合

- 基于RDD之间的依赖关系可以生成DAG
- DAG既描述了RDD之间的数据依赖关系，也描述了RDD之间的计算依赖关系。也就是说，DAG描述了Spark应用程序内的计算流程
- 这种依赖关系又称为“血缘关系”（Lineage）
- 当出现丢失分区数据时，可以快速进行分区重建，而不需要重新计算RDD的所有分区
- 当“血缘关系”较长，可以通过持久“血缘关系”来切断RDD

38

## Page 39

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

8.3.1 RDD简介

RDD是Spark中最基本的数据抽象，代表不可变、可分区和可并行计算的集合

- RDD可以缓存在内存中，极大提高了RDD的读取效率
- RDD对于迭代计算（数学计算、机器学习等）十分友好，这也是Spark要比Hadoop快的原因

39

## Page 40

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.3.2 工作原理

- RDD在每次转换操作后都会生成一个新的 RDD，因此RDD之间存在前后依赖关系
- RDD的依赖关系分为
- 窄依赖：父RDD的每个分区最多只对应子RDD的一个分区
- 宽依赖：子RDD的分区依赖于父RDD的多个分区

## Page 41

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 窄依赖

![img-15.jpeg](img-15.jpeg)

![img-16.jpeg](img-16.jpeg)

## Page 42

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 宽依赖

![img-17.jpeg](img-17.jpeg)
groupByKey

![img-18.jpeg](img-18.jpeg)
RDD11

## Page 43

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.3.2 工作原理

## 区分窄依赖和宽依赖的依据：是否包含Shuffle操作

- 窄依赖
- 父子RDD对应的分区数据的计算操作可以并行
- 宽依赖
- 子RDD需要等待父RDD的分区数据的Shuffle操作完成后才能执行

在Spark应用程序中，若能使用窄依赖实现，就尽量不要使用宽依赖

## Page 44

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.3.2 工作原理

- DAG调度器通过分析RDD中分区之间的依赖关系决定如何划分Stage
- 如果RDD都是窄依赖，那么可以将RDD计算划分到同一Stage中；
- 如果遇到宽依赖，须将任务划分到另一个Stage中

## Page 45

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.3.2 工作原理

## Stage划分

![img-19.jpeg](img-19.jpeg)

## Page 46

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.3.2 工作原理

## Stage

- Stage内部生成的RDD都是窄依赖
- Stage内部RDD计算可以并行
- Stage输出RDD和下一个Stage输入RDD之间是宽依赖
- 只有Stage之间的数据传输需要进行Shuffle

## Page 47

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.3.2 工作原理

## 根据输出的不同，Spark中Stage分为：

- ShuffleMapStage
- ShuffleMapStage的输出作为另一个Stage的输入
- ShuffleMapStage的输出必须经过Shuffle过程，并作为后续Stage的输入
- 一个DAG可以不包含ShuffleMapStage

- ResultStage
- ResultStage的输出直接产生最终的结果
- 一个DAG肯定包含ShuffleMapStage

## Page 48

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.3.2 工作原理

## Stage内部的数据传输

- Spark采用流水线方式进行Stage内部的数据传输
- Spark根据每个Stage输出RDD中分区个数决定启动的任务数量以执行该Stage中的操作

![img-20.jpeg](img-20.jpeg)

## Page 49

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.3.2 工作原理

## Stage之间的数据传输

- Stage之间的数据传输需要进行Shuffle，可分为Shuffle Write和Shuffle Read两个阶段
- Shuffle Write 阶段：ShuffleMapTask需要将输出RDD的记录按照分区函数partition，划分到相应的bucket中并物化到本地磁盘形成ShuffleblockFile
- Shuffle Read阶段：ShuffleMapTask或ResultTask根据partition函数读取相应的ShuffleblockFile，并将其存入缓冲区并继续后续的计算

## Page 50

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.3.2 工作原理

## Stage之间的数据传输

![img-21.jpeg](img-21.jpeg)

## Page 51

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.3.3 容错机制

- 从RDD计算的角度来看，Worker或Executor故障将导致部分RDD或RDD中的某些分区丢失
- 如何恢复丢失的RDD或RDD分区

## Page 52

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.3.3 容错机制

## RDD持久化

- Work进程可以管理一定大小的内存，主要用于存储RDD和计算过程中需要存储的其他辅助信息等
- 由于计算过程中会不断产生新的RDD，因此系统无法将所有的RDD均存储在内存中
- 一旦达到相应存储空间的阈值，Spark会使用置换算法（如LRU（Least Recently Used））对RDD缓存数据进行回收
- 如果不做任何声明，这些RDD将被直接丢弃
- 某些RDD后续可能会被再次使用，用户在编程时可以标明该RDD需要持久化

## Page 53

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.3.3 容错机制

## RDD持久化

- Spark应用程序可以使用`persist(Storagelevel)`方法或`cache()`方法将RDD持久化
- cache()只支持在内存中缓存数据
- persist()可以根据持久化级别配置不同的缓存级别
- 可以通过unpersist()方法释放缓存

## Page 54

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# Storagelevel的常见参数

- MEMORY_ONLY: 持久化到内存中，如果内存不足，则放弃持久化。这是RDD默认的持久化级别。
- MEMORY_AND_DISK: 先尝试持久化到内存中，如果内存不足，则将持久化到磁盘上。
- MEMORY_ONLY_SER: 以序列化的方式持久到内存中，RDD每个分区将被序列为一个字节数组
- MEMORY_AND_DISK_SER: 以序列化的方式持久到内存或磁盘中。
- DISK_ONLY: 持久化到磁盘中
- MEMORY_ONLY_2: 持久化方式同MEMORY_ONLY，但持久化的数据都会赋值一份，并保存到其他节点上。
- MEMORY_AND_DISK_2: 持久化方式同MEMORY_AND_DISK，但持久化的数据都会赋值一份，并保存到其他节点上。

## Page 55

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.3.3 容错机制

## 故障恢复

- SparkContext维护了记录RDD转换操作的DAG，即RDD的血缘关系(Linage)
- 当某一RDD的部分分区丢失时，其可以通过血缘关系(Linage)获取足够的信息以重新运算和恢复丢失的数据分区
- 结合计算过程中的某些持久化的RDD和血缘关系(Linage)即可进行故障恢复

## Page 56

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.3.3 容错机制

## 故障恢复

- 窄依赖
- 只需要重新计算丢失的分区数据

![img-22.jpeg](img-22.jpeg)

## Page 57

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.3.3 容错机制

## 故障恢复

- 宽依赖
- 需重新计算父RDD中的所有数据

![img-23.jpeg](img-23.jpeg)

## Page 58

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.3.3 容错机制

- 虽然RDD的持久化可以提供一定程度的多备份，但是多个备份的物理机器可能同时发生故障
- 极端情况下，系统只维护了当前产生的RDD并且Lineage较长，此时故障将导致系统的恢复过程根据RDD Lineage重新开始计算
- 如果RDD Lineage中存在大量宽依赖，则恢复过程代价较高
- 因此，在Lineage较长尤其是存在宽依赖时，可以在适当的时机设置检查点（checkpoint）

## Page 59

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.3.3 容错机制

## 检查点

- 检查点是Spark提供的一种基于快照的缓存机制
- 当需要计算的RDD过于复杂时，为了避免任务执行失败后重新计算之前的RDD，可以通过设置检查点将结果持久化到磁盘上或HDFS上

![img-24.jpeg](img-24.jpeg)

## Page 60

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.3.3 容错机制

## 检查点和持久化的区别

- 持久化
- 保存在本地的内存或磁盘
- 程序结束后会被清除或者调用unpersist方法清除
- 保存的是RDD，会保留RDD的血脉关系

- 检查点
- 保存在可靠的存储系统（HDFS）中
- 程序结束后依然存在，只能手动清除
- 保存的是RDD的数据，不包含血脉关系

## Page 61

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.3.4 编程示例

## 创建RDD

- 1. 通过并行集合（数组）创建RDD
- 可以调用SparkContext的parallelize方法，在Driver中一个已经存在的集合（如数组、列表）上创建。

![img-25.jpeg](img-25.jpeg)

## Page 62

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.3.4 编程示例

## 创建RDD

- 2. 从文件系统中加载数据创建RDD
- Spark采用textFile()方法来从文件系统中加载数据创建RDD
- 该方法把文件的URI作为参数，这个URI可以是：
- 本地文件系统的地址
- 或者是分布式文件系统HDFS的地址
- 或者是Amazon S3的地址等

## Page 63

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.3.4 编程示例

## 设置分区的个数

- 创建RDD时手动指定分区个数
- 在调用textFile()和parallelize()方法的时候手动指定分区个数
- 使用reparititon方法重新设置分区个数
- 通过转换操作得到新RDD时，直接调用repartition方法即可

## Page 64

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.3.4 编程示例

## 自定义分区方法

- Spark提供了自带的HashPartitioner（哈希分区）与RangePartitioner（区域分区），能够满足大多数应用场景的需求。
- Spark也支持自定义分区。
- 要实现自定义分区，需要定义一个类，这个自定义类需要继承org.apache.spark.Partitioner类，并实现下面三个方法：
- numPartitions: Int 返回创建出来的分区数
- getPartition(key: Any): Int 返回给定键的分区编号（0到numPartitions-1）
- equals() Java判断相等性的标准方法

## Page 65

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.3.4 编程示例

## 一个简单的RDD示例

```java
object RDDdemo {
def main(args:Array[String]): Unit = {
val conf = new SparkConf().setAppName("rdddemo").setMaster("local");
// val conf = new SparkConf().setAppName("rdddemo");
val sc = new SparkContext(conf);
}

var list = List("beijing", "beijing", "beijing", "shanghai", "shanghai", "tianjin", "tianjin");
val rdd = sc.paralleize(list);

val result = rdd.map((_,1)).reduceByKey((_+_));
}

result.collect().foreach(x =&gt; println(x));
sc.stop();
}
```

## Page 66

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.3.4 编程示例

## 一个简单的RDD示例

```java
object RDDdemo2 {
def main(args:Array[String]): Unit = {
val conf = new SparkConf().setAppName("rdddemo").setMaster("local");
// val conf = new SparkConf().setAppName("rdddemo");
val sc = new SparkContext(conf);
val rdd = sc.textFile("hdfs://localhost:9000/user/hadoop/word.txt");
// val rdd = sc.textFile("/user/hadoop/word.txt")
// val rdd = sc.textFile("word.txt")
val result = rdd.map((_,1)).reduceByKey((_+_));
result.collect().foreach(x =&gt; println(x));
sc.stop();
}
}

## Page 67

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.3.4 编程示例

## Spark广播变量

- Spark支持使用有效的广播算法分发广播变量，以降低通信成本
- Spark广播变量会自动广播每个阶段的任务所需的公共数据
- 这些数据已经以序列化形式存在系统中；在任务运行期间，当需要这些数据时，对它们进行反序列化操作即可

![img-26.jpeg](img-26.jpeg)

## Page 68

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.3.4 编程示例

## 广播变量的优势

- 可以节省内存，避免内存溢出问题
- 减少网络传输数据量
- 广播变量把数据缓存在内存中，提高了计算速度

![img-27.jpeg](img-27.jpeg)

## Page 69

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.3.4 编程示例

## 使用广播变量

- SparkContext通过newBroadcast()方法创建广播变量，并通过BroadcastManager将广播变量发送到每个节点
- 当广播变量不再使用时，使用unpersist()方法释放广播变量

```java
object RDDdemo3 {
def main(args:Array[String]): Unit = {
val conf = new SparkConf().setAppName("rdddemo").setMaster("local");
// val conf = new SparkConf().setAppName("rdddemo");
val sc = new SparkContext(conf);
}

val bc = sc.broadcast(Array(1,2,3));
val rdd = bc.value;
rdd.foreach(x =&gt; println(x));

bc.unpersist();
bc.destroy();
sc.stop();
}
}

## Page 70

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 主要内容

8.1 Spark概述
8.2 Spark基础
8.3 Spark RDD
8.4 Spark SQL
8.5 Spark流计算
8.6 Spark机器学习

70

## Page 71

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.4 Spark SQL

- 8.4.1 Spark SQL基础
- 8.4.2 Spark SQL编程
- 8.4.3 用户自定义函数
- 8.4.4 Spark SQL执行过程

71

## Page 72

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.4.1 Spark SQL基础

- Spark SQL是Spark用来处理结构化数据的一个模块，它提供了2个编程抽象：DataFrame和DataSet，并且作为分布式SQL查询引擎的作用

72

## Page 73

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.4.1 Spark SQL基础

## DataFrame

- DataFrame是一种分布式数据集合，其中的每一条数据都由多个字段组成
- RDD是分布式对象的集合，但对象内部结构对于RDD而言却是不可知的
- DataFrame是一种以RDD为基础的分布式数据集，提供了详细的结构信息

|  Person  |
| --- |
|  Person  |
|  Person  |
|  Name | Age | Height  |
| --- | --- | --- |
|  String | Int | Double  |
|  String | Int | Double  |
|  String | Int | Double  |
|  Person  |
| --- |
|  Person  |
|  Person  |
|  String | Int | Double  |
| --- | --- | --- |
|  String | Int | Double  |
|  String | Int | Double  |

RDD[Person]
DataFrame

## Page 74

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.4.1 Spark SQL基础

## DataFrame

- DataFrame 可以从很多数据源加载数据
- DataFrame支持多种文件格式

![img-28.jpeg](img-28.jpeg)

## Page 75

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.4.1 Spark SQL基础

- DataFrame是为数据提供了Schema的视图，可以把它当做一张二维的结构化表来对待。
- 查询计划会通过Spark Catalyst优化器进行优化，所以DataFrame性能上比RDD要高。

75

## Page 76

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.4.1 Spark SQL基础

基于Spark SQL，开发人员可以通过简单的SQL实现复杂的大数据计算

- 在Spark内部，Spark SQL会将SQL语句转换为RDD之间的操作，然后提交到集群并执行

![img-29.jpeg](img-29.jpeg)

## Page 77

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.4.1 Spark SQL基础

## DataSet

- 是Dataframe API的一个扩展，是Spark最新的数据抽象
- DataSet是强类型的
- 用户友好的API风格，既具有类型安全检查也具有Dataframe的查询优化特性
- Dataframe是Dataset的特列，DataFrame=Dataset[Row]

![img-30.jpeg](img-30.jpeg)

![img-31.jpeg](img-31.jpeg)
RDD[Person]

![img-32.jpeg](img-32.jpeg)

![img-33.jpeg](img-33.jpeg)
DataFrame

![img-34.jpeg](img-34.jpeg)

![img-35.jpeg](img-35.jpeg)
DataSet[Person]

## Page 78

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

78

# 8.4.2 Spark SQL编程

## SparkSession

- SparkSession是Spark最新的SQL查询起始点
- SparkSession是创建DataFrame和执行SQL的入口
- SparkSession内部封装了SparkContext，所以计算实际上是由SparkContext完成的。

## Page 79

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

79

# 8.4.2 Spark SQL编程

- 创建DataFrame有三种方式
- 通过Spark的数据源进行创建
- 从一个存在的RDD进行转换
- 从Hive Table进行查询返回

## Page 80

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

8.4.2 Spark SQL编程

创建DataFrame有三种方式
- 通过Spark的数据源进行创建

```java
object SQLSimple {
def main(args:Array[String]): Unit = {
val spark=SparkSession.builder().appName("sqldemo").master("local").getOrCreate();
val df=spark.read.json("hdfs://localhost:9000/user/hadoop/person.json");
df.show();
df.write.csv("hdfs://localhost:9000/user/hadoop/person.csv");
}
}
```

```txt
{"name":"zhangsan","age":20}
{"name":"lisi","age":24}
{"name":"wangwu","age":27}
```

```txt
+---+---+
|age| name|
+---+---+
| 20|zhangsan|
| 24| lisi|
| 27| wangwu|
+---+---+
```

80

## Page 81

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

8.4.2 Spark SQL编程

创建DataFrame有三种方式

- 从一个存在的RDD进行转换
- 利用反射机制推断RDD模式

```java
object SQLSimple2 {
case class Person(name:String,age:Int);

def main(args:Array[String]):Unit = {
val spark=SparkSession.builder().appName("sqldemo").master("local").getOrCreate();

import spark.implicits.;
val df=spark.sparkContext.textFile("hdfs://localhost:9000/user/hadoop/person.txt").map(_.split(",").map(line=&gt;Person(line(0),line(1).toInt)).toDF();
df.show();
}
}
```

```txt
zhangsan,20
lisi,24
wangwu,24
```

```txt
+---+---+
| name|age|
+---+---+
| zhangsan| 20|
| lisi| 24|
| wangwu| 24|
+---+---+
```

81

## Page 82

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

8.4.2 Spark SQL编程

创建DataFrame有三种方式

- 从一个存在的RDD进行转换
- 使用编程方式定义RDD模式

```java
object SQLSimple3 {
def main(args:Array[String]): Unit = {
val spark=SparkSession.builder().appName("sqldemo").master("local").getOrCreate();
val fields=Array(StructField("name", StringType, true),
StructField("age", IntegerType, true));
val schema=StructType(fields);
val rdd=spark.sparkContext.textFile("hdfs://localhost:9000/user/hadoop/person.txt")
.map(_.split(", ").map(line=&gt;Row(line(0), line(1).toInt));
val df=spark.createDataFrame(rdd, schema);
df.show();
}
}
```

zhangsan,20
lisi,24
wangwu,24

82

## Page 83

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

8.4.2 Spark SQL编程

## Spark SQL视图操作

- 在实际开发中，若将DataFrame映射为视图，就可以直接在视图上执行SQL操作
- Spark视图分为临时视图和全局临时视图
- 临时视图:仅限会话范围，如果创建临时视图的会话终止，临时视图就会消失
- 全局临时视图：能在会话之间共享并使其保持活动状态，直到Spark应用程序终止
- 全局临时视图与系统保留数据库global_temp绑定在一起

83

## Page 84

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

8.4.2 Spark SQL编程

# Spark SQL视图操作

```txt
def main(args:Array[String]):Unit = [0]
val spark=SparkSession.builder().appName("sqldemo").master("local").getOrCreate();
val df=spark.read.json("hdfs://localhost:9000/user/hadoop/person.json");

df.createOrReplaceTempView("people");
spark.sql("SELECT * FROM people WHERE age&gt;20").show();

df.createOrReplaceGlobalTempView("people2");
spark.sql("SELECT * FROM global_temp.people2 WHERE age&gt;20").show();
}
```

84

## Page 85

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

8.4.2 Spark SQL编程

## Dataset

- RDD转Dataset
- DataFrame转DataSet
- DataSet转DataFrame

85

## Page 86

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

8.4.2 Spark SQL编程

## Dataset

- RDD转Dataset
- DataFrame转DataSet
- DataSet转DataFrame

```java
object SQLSimple4 {
case class Person(name:String,age:Int);

def main(args:Array[String]):Unit = {
val spark=SparkSession.builder().appName("sqldemo").master("local").getOrCreate();

import spark.implicits.;
val rdd=spark.sparkContext.textFile("hdfs://localhost:9000/user/hadoop/person.txt").map(_.split(",").map(line=&gt;Person(line(0),line(1).toInt));
val df1=rdd.toDF();
val ds1=rdd.toDS();
val ds2=df1.as[Person];
val df2=ds1.toDF();
}
}
```

86

## Page 87

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.4.3 用户自定义函数

## 除使用Spark内置函数之外，用户也可以自定义函数，进而满足更复杂的函数需求

- 例：用户自定义函数，实现简单的age++功能

```txt
val plusOne=udf((x:Int)=&gt; x+1)
spark.udf.register("plusOne",plusOne)
spark.sql("select age,plusOne(age) age_1,name from people").show()
```

## 具体过程如下

- 定义一个用户自定义函数plusOne()
- 使用spark.udf.register("plusOne", plusOne)将自定义函数注册到Spark的UDF中
- 在Spark SQL中使用注册的plusOne函数

87

## Page 88

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.4.3 用户自定义函数

## 除使用Spark内置函数之外，用户也可以自定义函数，进而满足更复杂的函数需求

- 例：实现更复杂的功能，可以根据传入的步长对数据进行操作

```r
spark.udf.register("age_plus", (age:Int, step:Int) =&gt; {age+step})
spark.sql("select age, age_plus(age, 10) age_1, name from people").show()
```

88

## Page 89

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.4.4 Spark SQL执行过程

在进行Spark SQL查询时，既可以使用Spark提供的SQL语法，也可以使用Hive SQL语法（HiveQL），也可以使用DataFrame DSL

- SQL语句会通过Spark解析器转换为对DataFrame的操作
- HiveQL会通过Hive解析器将转换对DataFrame的操作
- DataFrame DSL本身就是对DataFrame操作

89

## Page 90

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.4.4 Spark SQL执行过程

- Spark在接收到转换后的DataFrame操作后，还需要使用Spark Catalyst优化器将执行过程优化为底层的Spark操作

![img-36.jpeg](img-36.jpeg)

## Page 91

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.4.4 Spark SQL执行过程

- Catalyst是Spark SQL最核心的部分，主要负责SQL语句的解析、绑定、优化以及生成物理的执行计划
- Catalyst主要由解析器、分析器、优化器和计划器4个模块组成

- 解析器: 同时兼容 ANSISQL2003 标准和 HiveQL，用于将 SQL、Dataset 和 DataFrame 转换成一棵未经解析(unresolved)的树，这棵树在 Spark 中被称为逻辑计划，它是对用户程序的一种抽象。
- 分析器: 作用是利用目录树中的信息，对解析器生成的树进行解析。分析器由一系列规则组成，每一条规则负责一项检查或转换操作，例如解析SQL中的表名、列名，同时判它们是否存在。通过分析器，我们可以得到解析后的逻辑计划。
- 优化器: 作用是对解析完的逻辑计划进行结构优化。优化过程通过一系列的规则来完成的，常用的规则有谓词下推(predicate pushdown)、列裁剪(column pruning)、连接重排序(join reordering)等。此外, SparkSQL, 还提供了基于成本的优化器(cost-based optimizer), 以基于数据分布情况自动生成最优的执行计划。
- 计划器: 作用是将优化后的逻辑计划转换成物理计划。计划器由一系列的策略组成，每个策略都能将某个逻辑算子转换成对应的物理执行算子，并最终转换成RDD操作。需要说明的是，在转换过程中，一个逻辑算子可能对应多个物理算子的实现，例如连接的实现就有 SortMergeJoin 和 BroadcastHashJoin。在实践中，我们需要基于成本模型(cost model)来选择最优的算子。

91

## Page 92

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.4.4 Spark SQL执行过程

整个 Catalyst 框架拥有良好的可扩展性，开发人员可以根据不同的需求，灵活地添加自己的语法、解析规则、优化规则和转换策略。

## Catalyst执行流程

![img-37.jpeg](img-37.jpeg)

## Page 93

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

8.4.4 Spark SQL执行过程

- Catalyst执行流程
- (1) 使用解析器将 SQL 语句解析为语法树，语法树又称为解析后的逻辑计划。
- (2) 使用分析器和目录树中的表信息分析解析后的逻辑计划。
- (3) 使用各种基于规则的优化策略进行深入优化，得到优化后的逻辑计划。优化后的逻辑计划依然是逻辑计划，Spark 不能直接执行。
- (4) 将优化后的逻辑计划转换为物理计划，然后基于物理计划将 SQL 语句转换为 RDD 计算并执行。
- 经过上述操作后，就完成了从用户编写的 SQL 语句到 Spark 内部 RDD 的具体操作逻辑的转换。

93

## Page 94

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

# 8.4.4 Spark SQL执行过程

## 具体示例：基于代码分析SparkSQL执行计划

- spark.sql(" SELECT * FROM people where age &gt;=20") .explain(true)

![img-38.jpeg](img-38.jpeg)

## Page 95

*Metadata:*
- Width: 2000
- Height: 1500
- DPI: 200

95

謝謝！

---

## Extracted Images

Total images found: 39

### Image 1

- **ID**: `img-0.jpeg`
- **Position**: (386, 424) to (1354, 804)
- **Size**: 968 × 380 pixels

### Image 2

- **ID**: `img-1.jpeg`
- **Position**: (382, 899) to (1348, 1054)
- **Size**: 966 × 155 pixels

### Image 3

- **ID**: `img-2.jpeg`
- **Position**: (382, 1101) to (1074, 1324)
- **Size**: 692 × 223 pixels

### Image 4

- **ID**: `img-3.jpeg`
- **Position**: (182, 586) to (1602, 1234)
- **Size**: 1420 × 648 pixels

### Image 5

- **ID**: `img-4.jpeg`
- **Position**: (352, 780) to (1442, 1288)
- **Size**: 1090 × 508 pixels

### Image 6

- **ID**: `img-5.jpeg`
- **Position**: (72, 523) to (858, 997)
- **Size**: 786 × 474 pixels

### Image 7

- **ID**: `img-6.jpeg`
- **Position**: (738, 931) to (1966, 1461)
- **Size**: 1228 × 530 pixels

### Image 8

- **ID**: `img-7.jpeg`
- **Position**: (68, 556) to (856, 1045)
- **Size**: 788 × 489 pixels

### Image 9

- **ID**: `img-8.jpeg`
- **Position**: (782, 987) to (1980, 1494)
- **Size**: 1198 × 507 pixels

### Image 10

- **ID**: `img-9.jpeg`
- **Position**: (228, 560) to (1770, 1375)
- **Size**: 1542 × 815 pixels

### Image 11

- **ID**: `img-10.jpeg`
- **Position**: (220, 412) to (1604, 1416)
- **Size**: 1384 × 1004 pixels

### Image 12

- **ID**: `img-11.jpeg`
- **Position**: (94, 478) to (1778, 1497)
- **Size**: 1684 × 1019 pixels

### Image 13

- **ID**: `img-12.jpeg`
- **Position**: (114, 468) to (1758, 1485)
- **Size**: 1644 × 1017 pixels

### Image 14

- **ID**: `img-13.jpeg`
- **Position**: (380, 760) to (1646, 1347)
- **Size**: 1266 × 587 pixels

### Image 15

- **ID**: `img-14.jpeg`
- **Position**: (230, 706) to (1780, 1153)
- **Size**: 1550 × 447 pixels

### Image 16

- **ID**: `img-15.jpeg`
- **Position**: (432, 430) to (886, 757)
- **Size**: 454 × 327 pixels

### Image 17

- **ID**: `img-16.jpeg`
- **Position**: (966, 528) to (1426, 1195)
- **Size**: 460 × 667 pixels

### Image 18

- **ID**: `img-17.jpeg`
- **Position**: (646, 252) to (1246, 646)
- **Size**: 600 × 394 pixels

### Image 19

- **ID**: `img-18.jpeg`
- **Position**: (634, 778) to (1250, 1342)
- **Size**: 616 × 564 pixels

### Image 20

- **ID**: `img-19.jpeg`
- **Position**: (364, 415) to (1472, 1408)
- **Size**: 1108 × 993 pixels

### Image 21

- **ID**: `img-20.jpeg`
- **Position**: (348, 690) to (1616, 1377)
- **Size**: 1268 × 687 pixels

### Image 22

- **ID**: `img-21.jpeg`
- **Position**: (288, 443) to (1552, 1359)
- **Size**: 1264 × 916 pixels

### Image 23

- **ID**: `img-22.jpeg`
- **Position**: (474, 612) to (1358, 1383)
- **Size**: 884 × 771 pixels

### Image 24

- **ID**: `img-23.jpeg`
- **Position**: (1154, 286) to (1810, 1363)
- **Size**: 656 × 1077 pixels

### Image 25

- **ID**: `img-24.jpeg`
- **Position**: (522, 745) to (1552, 1435)
- **Size**: 1030 × 690 pixels

### Image 26

- **ID**: `img-25.jpeg`
- **Position**: (614, 687) to (1254, 1165)
- **Size**: 640 × 478 pixels

### Image 27

- **ID**: `img-26.jpeg`
- **Position**: (586, 904) to (1534, 1450)
- **Size**: 948 × 546 pixels

### Image 28

- **ID**: `img-27.jpeg`
- **Position**: (332, 697) to (1612, 1411)
- **Size**: 1280 × 714 pixels

### Image 29

- **ID**: `img-28.jpeg`
- **Position**: (254, 633) to (1520, 1248)
- **Size**: 1266 × 615 pixels

### Image 30

- **ID**: `img-29.jpeg`
- **Position**: (474, 682) to (1534, 1384)
- **Size**: 1060 × 702 pixels

### Image 31

- **ID**: `img-30.jpeg`
- **Position**: (236, 927) to (678, 1102)
- **Size**: 442 × 175 pixels

### Image 32

- **ID**: `img-31.jpeg`
- **Position**: (236, 1117) to (678, 1291)
- **Size**: 442 × 174 pixels

### Image 33

- **ID**: `img-32.jpeg`
- **Position**: (756, 849) to (1198, 1102)
- **Size**: 442 × 253 pixels

### Image 34

- **ID**: `img-33.jpeg`
- **Position**: (756, 1117) to (1198, 1291)
- **Size**: 442 × 174 pixels

### Image 35

- **ID**: `img-34.jpeg`
- **Position**: (1254, 849) to (1796, 1102)
- **Size**: 542 × 253 pixels

### Image 36

- **ID**: `img-35.jpeg`
- **Position**: (1254, 1117) to (1796, 1291)
- **Size**: 542 × 174 pixels

### Image 37

- **ID**: `img-36.jpeg`
- **Position**: (352, 571) to (1644, 1365)
- **Size**: 1292 × 794 pixels

### Image 38

- **ID**: `img-37.jpeg`
- **Position**: (48, 666) to (1888, 1302)
- **Size**: 1840 × 636 pixels

### Image 39

- **ID**: `img-38.jpeg`
- **Position**: (106, 480) to (1818, 1338)
- **Size**: 1712 × 858 pixels

---

*Generated by Mistral OCR MCP Server*
